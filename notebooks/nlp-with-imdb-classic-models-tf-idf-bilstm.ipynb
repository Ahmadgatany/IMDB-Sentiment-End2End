{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be0853c",
   "metadata": {
    "papermill": {
     "duration": 0.005581,
     "end_time": "2025-07-01T00:33:29.930635",
     "exception": false,
     "start_time": "2025-07-01T00:33:29.925054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"1\"></a>\n",
    "# <div style=\"text-align:center; background: #03045e; padding: 7px; border-radius:10px 10px; font-size: 1.5em; color: #48cae4; cursor: pointer;font-family: cursive;\"><b> 1. Introduction </b></div>\n",
    "\n",
    "Sentiment analysis is one of the most common and impactful applications of Natural Language Processing (NLP).  \n",
    "It enables machines to understand the emotional tone behind text ‚Äî a core component in tasks like product reviews, social media monitoring, and customer feedback analysis.\n",
    "\n",
    "In this notebook, we dive into the IMDB dataset, which contains 50,000 labeled movie reviews, to build and compare two classic NLP pipelines:\n",
    "\n",
    "- A **Logistic Regression** model with **TF-IDF** features  \n",
    "- A **BiLSTM** model powered by **Word2Vec** embeddings\n",
    "\n",
    "Through this exploration, you‚Äôll see how even traditional models can produce meaningful results in sentiment classification ‚Äî and how preprocessing, feature extraction, and model selection all come together in an end-to-end NLP workflow.\n",
    "\n",
    "üîÑ In a follow-up notebook (**NLP with IMDB: Classic Models vs. Transformers**), we will compare these results with predictions from a **Transformer-based model (DeBERTa)**, implemented in a separate notebook (**NLP with IMDB: Transformers - DeBERTa**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442d00a5",
   "metadata": {
    "papermill": {
     "duration": 0.004406,
     "end_time": "2025-07-01T00:33:29.940167",
     "exception": false,
     "start_time": "2025-07-01T00:33:29.935761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![Simple RNN](https://miro.medium.com/v2/resize:fit:828/format:webp/1*3ltsv1uzGR6UBjZ6CUs04A.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bde09d",
   "metadata": {
    "papermill": {
     "duration": 0.004506,
     "end_time": "2025-07-01T00:33:29.949201",
     "exception": false,
     "start_time": "2025-07-01T00:33:29.944695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"text-align:center; background: #03045e; padding: 7px; border-radius:10px 10px; font-size: 1.5em; color: #e3f2fd; cursor: pointer;font-family: cursive;\"><b> 2. Importing Libraries </b></div>\n",
    "\n",
    "In this section, we import all the libraries required for data processing, feature extraction, and model building. These libraries help us perform tasks like loading and manipulating data, tokenizing text, building neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95581afe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T00:33:29.959536Z",
     "iopub.status.busy": "2025-07-01T00:33:29.959155Z",
     "iopub.status.idle": "2025-07-01T00:33:46.770842Z",
     "shell.execute_reply": "2025-07-01T00:33:46.770073Z"
    },
    "papermill": {
     "duration": 16.819184,
     "end_time": "2025-07-01T00:33:46.772805",
     "exception": false,
     "start_time": "2025-07-01T00:33:29.953621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLP preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Scikit-learn for splitting data, evaluation and models \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# TensorFlow/Keras for deep learning (BiLSTM)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Bidirectional, Dense, Dropout, Masking\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58069385",
   "metadata": {
    "papermill": {
     "duration": 0.004691,
     "end_time": "2025-07-01T00:33:46.782658",
     "exception": false,
     "start_time": "2025-07-01T00:33:46.777967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"text-align:center; background: #03045e; padding: 7px; border-radius:10px 10px; font-size: 1.5em; color: #e3f2fd; cursor: pointer;font-family: cursive;\"><b> 3. Loading the Dataset </b></div>\n",
    "\n",
    "In this step, we load the IMDB dataset, which contains 50,000 movie reviews labeled as either positive or negative.  \n",
    "Each review is stored as a text entry along with its sentiment label.\n",
    "\n",
    "We use `pandas.read_csv()` to load the dataset into a DataFrame for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd1c40d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T00:33:46.793922Z",
     "iopub.status.busy": "2025-07-01T00:33:46.792821Z",
     "iopub.status.idle": "2025-07-01T00:33:48.215062Z",
     "shell.execute_reply": "2025-07-01T00:33:48.214074Z"
    },
    "papermill": {
     "duration": 1.429725,
     "end_time": "2025-07-01T00:33:48.217002",
     "exception": false,
     "start_time": "2025-07-01T00:33:46.787277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Show the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83cb72c",
   "metadata": {
    "papermill": {
     "duration": 0.004844,
     "end_time": "2025-07-01T00:33:48.227054",
     "exception": false,
     "start_time": "2025-07-01T00:33:48.222210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"text-align:center; background: #03045e; padding: 7px; border-radius:10px 10px; font-size: 1.5em; color: #e3f2fd; cursor: pointer;font-family: cursive;\"><b> 4. Data Preprocessing </b></div>\n",
    "\n",
    "Text data in its raw form usually contains noise such as HTML tags, punctuation, numbers, links, and stopwords. These elements do not contribute meaningfully to the model and may negatively affect its performance.\n",
    "\n",
    "In this step, we define a `preprocess_text()` function to clean the reviews by:\n",
    "\n",
    "- Converting all text to lowercase.\n",
    "- Removing HTML tags.\n",
    "- Removing URLs.\n",
    "- Removing non-alphabetic characters.\n",
    "- Removing extra spaces.\n",
    "- Removing stopwords (commonly used words like \"the\", \"is\", etc. that carry little meaning in classification tasks).\n",
    "\n",
    "Finally, we apply this function to the review column and create a new column called `clean_review`.\n",
    "\n",
    "**Why preprocessing is important in NLP:**\n",
    "- It reduces noise in the data.\n",
    "- Helps models focus on meaningful patterns.\n",
    "- Improves accuracy and generalization of NLP models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "967c576f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T00:33:48.238107Z",
     "iopub.status.busy": "2025-07-01T00:33:48.237756Z",
     "iopub.status.idle": "2025-07-01T00:33:54.893244Z",
     "shell.execute_reply": "2025-07-01T00:33:54.892329Z"
    },
    "papermill": {
     "duration": 6.663273,
     "end_time": "2025-07-01T00:33:54.895248",
     "exception": false,
     "start_time": "2025-07-01T00:33:48.231975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>one reviewers mentioned watching oz episode yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>basically theres family little boy jake thinks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  One of the other reviewers has mentioned that ...   \n",
       "1  A wonderful little production. <br /><br />The...   \n",
       "2  I thought this was a wonderful way to spend ti...   \n",
       "3  Basically there's a family where a little boy ...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
       "\n",
       "                                        clean_review  \n",
       "0  one reviewers mentioned watching oz episode yo...  \n",
       "1  wonderful little production filming technique ...  \n",
       "2  thought wonderful way spend time hot summer we...  \n",
       "3  basically theres family little boy jake thinks...  \n",
       "4  petter matteis love time money visually stunni...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove stopwords\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing to the 'review' column\n",
    "df['clean_review'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "# Show sample after preprocessing\n",
    "df[['review', 'clean_review']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad1118d",
   "metadata": {
    "papermill": {
     "duration": 0.004843,
     "end_time": "2025-07-01T00:33:54.905396",
     "exception": false,
     "start_time": "2025-07-01T00:33:54.900553",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Convert Sentiment Labels\n",
    "\n",
    "The sentiment labels in the dataset are in text form: \"positive\" or \"negative\".  \n",
    "To make them usable for machine learning models, we map:\n",
    "\n",
    "- \"negative\" ‚Üí 0  \n",
    "- \"positive\" ‚Üí 1\n",
    "\n",
    "This numeric format is required for most classification algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ce059fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T00:33:54.916417Z",
     "iopub.status.busy": "2025-07-01T00:33:54.915827Z",
     "iopub.status.idle": "2025-07-01T00:33:54.923437Z",
     "shell.execute_reply": "2025-07-01T00:33:54.922785Z"
    },
    "papermill": {
     "duration": 0.014795,
     "end_time": "2025-07-01T00:33:54.924989",
     "exception": false,
     "start_time": "2025-07-01T00:33:54.910194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Keep only Positive and Negative samples\n",
    "df['sentiment'] = df['sentiment'].map({'negative': 0, 'positive': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c91d98a",
   "metadata": {
    "papermill": {
     "duration": 0.004743,
     "end_time": "2025-07-01T00:33:54.934777",
     "exception": false,
     "start_time": "2025-07-01T00:33:54.930034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Split the Dataset\n",
    "\n",
    "We divide the dataset into three parts:\n",
    "\n",
    "- **Training set (64%)**: Used to train the model.\n",
    "- **Validation set (16%)**: Used to tune model parameters and prevent overfitting.\n",
    "- **Test set (20%)**: Used to evaluate final performance (will be used in a separate notebook).\n",
    "\n",
    "We use `train_test_split` from scikit-learn and apply stratified sampling to maintain equal class distribution across splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b030544b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T00:33:54.945559Z",
     "iopub.status.busy": "2025-07-01T00:33:54.945076Z",
     "iopub.status.idle": "2025-07-01T00:33:54.991590Z",
     "shell.execute_reply": "2025-07-01T00:33:54.990552Z"
    },
    "papermill": {
     "duration": 0.053691,
     "end_time": "2025-07-01T00:33:54.993287",
     "exception": false,
     "start_time": "2025-07-01T00:33:54.939596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 32000\n",
      "Validation size: 8000\n",
      "Test size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Split into train (80%) and test (20%)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# Step 2: Split train into train (80% of 80%) and val (20% of 80%) ‚Üí 64% train, 16% val\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['sentiment'])\n",
    "\n",
    "# Show sizes\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Validation size: {len(val_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a323709c",
   "metadata": {
    "papermill": {
     "duration": 0.004786,
     "end_time": "2025-07-01T00:33:55.003590",
     "exception": false,
     "start_time": "2025-07-01T00:33:54.998804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"text-align:center; background: #03045e; padding: 7px; border-radius:10px 10px; font-size: 1.5em; color: #e3f2fd; cursor: pointer;font-family: cursive;\"><b> 5. Tokenization</b></div>\n",
    "\n",
    "After cleaning the text, we perform **tokenization**, which is the process of splitting each review (a string) into individual words (called tokens).\n",
    "\n",
    "We use `nltk.word_tokenize` to convert each cleaned review into a list of words.  \n",
    "This results in a new column `tokens` that holds the tokenized version of each review.\n",
    "\n",
    "### Why is Tokenization Important in NLP?\n",
    "\n",
    "- It transforms raw text into structured units (words or subwords) that can be processed by models.\n",
    "- Most NLP models operate on word-level inputs, so tokenization is a **critical first step**.\n",
    "- It enables later steps like embedding, padding, and sequence modeling (e.g., using RNNs or Transformers).\n",
    "\n",
    "Tokenization is the bridge between raw text and numerical representation of language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71c95d8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T00:33:55.014624Z",
     "iopub.status.busy": "2025-07-01T00:33:55.014350Z",
     "iopub.status.idle": "2025-07-01T00:34:19.344283Z",
     "shell.execute_reply": "2025-07-01T00:34:19.343457Z"
    },
    "papermill": {
     "duration": 24.337656,
     "end_time": "2025-07-01T00:34:19.346107",
     "exception": false,
     "start_time": "2025-07-01T00:33:55.008451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_review</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26680</th>\n",
       "      <td>oh yes agree others describe appalling acting ...</td>\n",
       "      <td>[oh, yes, agree, others, describe, appalling, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16648</th>\n",
       "      <td>basic hook lincoln slow slowness represents th...</td>\n",
       "      <td>[basic, hook, lincoln, slow, slowness, represe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29967</th>\n",
       "      <td>utter trash im huge fan cusacks sole reason wa...</td>\n",
       "      <td>[utter, trash, im, huge, fan, cusacks, sole, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34122</th>\n",
       "      <td>meet cosmo jason priestley nerdy young bookie ...</td>\n",
       "      <td>[meet, cosmo, jason, priestley, nerdy, young, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>dont know people criticise show muchit great f...</td>\n",
       "      <td>[dont, know, people, criticise, show, muchit, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            clean_review  \\\n",
       "26680  oh yes agree others describe appalling acting ...   \n",
       "16648  basic hook lincoln slow slowness represents th...   \n",
       "29967  utter trash im huge fan cusacks sole reason wa...   \n",
       "34122  meet cosmo jason priestley nerdy young bookie ...   \n",
       "823    dont know people criticise show muchit great f...   \n",
       "\n",
       "                                                  tokens  \n",
       "26680  [oh, yes, agree, others, describe, appalling, ...  \n",
       "16648  [basic, hook, lincoln, slow, slowness, represe...  \n",
       "29967  [utter, trash, im, huge, fan, cusacks, sole, r...  \n",
       "34122  [meet, cosmo, jason, priestley, nerdy, young, ...  \n",
       "823    [dont, know, people, criticise, show, muchit, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply word_tokenize to the cleaned review column\n",
    "train_df['tokens'] = train_df['clean_review'].apply(word_tokenize)\n",
    "val_df['tokens'] = val_df['clean_review'].apply(word_tokenize)\n",
    "test_df['tokens'] = test_df['clean_review'].apply(word_tokenize)\n",
    "\n",
    "# Show sample tokens\n",
    "train_df[['clean_review', 'tokens']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18b6f1",
   "metadata": {
    "papermill": {
     "duration": 0.004955,
     "end_time": "2025-07-01T00:34:19.356571",
     "exception": false,
     "start_time": "2025-07-01T00:34:19.351616",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"text-align:center; background: #03045e; padding: 7px; border-radius:10px 10px; font-size: 1.5em; color: #e3f2fd; cursor: pointer;font-family: cursive;\"><b> 6. Text Representation Methods </b></div>\n",
    "\n",
    "Before feeding text into machine learning or deep learning models, we must first convert it into a numerical format. This step is known as **text representation**, and it‚Äôs one of the most important steps in any NLP pipeline.\n",
    "\n",
    "In this section, we explore two common techniques:\n",
    "- **TF-IDF Feature Extraction**: A classical method that assigns importance to words based on their frequency.\n",
    "- **Word2Vec Embedding**: A neural embedding technique that captures the semantic meaning of words.\n",
    "\n",
    "Each method has its advantages and is suited to different types of models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b94523",
   "metadata": {
    "papermill": {
     "duration": 0.004872,
     "end_time": "2025-07-01T00:34:19.366437",
     "exception": false,
     "start_time": "2025-07-01T00:34:19.361565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6.1 TF-IDF Feature Extraction\n",
    "\n",
    "TF-IDF (Term Frequency ‚Äì Inverse Document Frequency) is a classic technique used to convert text data into numerical features.\n",
    "\n",
    "We use the `TfidfVectorizer` from Scikit-learn to:\n",
    "- Join tokens back into full sentences (as TF-IDF works on raw text).\n",
    "- Fit the vectorizer on the training data.\n",
    "- Transform all text into a matrix of numerical values.\n",
    "\n",
    "### Why use TF-IDF?\n",
    "\n",
    "- It highlights **important words** in each document while down-weighting common words across all documents.\n",
    "- It produces a **sparse matrix** representation useful for traditional ML models like Logistic Regression or SVM.\n",
    "- It is **fast and interpretable**, making it suitable for baseline models.\n",
    "\n",
    "TF-IDF doesn‚Äôt understand word meaning or context, but it‚Äôs often surprisingly effective for many NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbbb3650",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T00:34:19.377784Z",
     "iopub.status.busy": "2025-07-01T00:34:19.377224Z",
     "iopub.status.idle": "2025-07-01T00:34:24.183834Z",
     "shell.execute_reply": "2025-07-01T00:34:24.183102Z"
    },
    "papermill": {
     "duration": 4.814415,
     "end_time": "2025-07-01T00:34:24.185854",
     "exception": false,
     "start_time": "2025-07-01T00:34:19.371439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Join the tokens back into full sentences (as TF-IDF expects raw text input)\n",
    "train_texts = train_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "val_texts = val_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "test_texts = test_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Initialize TF-IDF vectorizer with a maximum of 5000 features\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit the vectorizer on training data and transform it\n",
    "X_train_tfidf = tfidf.fit_transform(train_texts)\n",
    "\n",
    "# Transform validation and test sets using the fitted vectorizer\n",
    "X_val_tfidf = tfidf.transform(val_texts)\n",
    "X_test_tfidf = tfidf.transform(test_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8469d4",
   "metadata": {
    "papermill": {
     "duration": 0.005171,
     "end_time": "2025-07-01T00:34:24.196572",
     "exception": false,
     "start_time": "2025-07-01T00:34:24.191401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6.2 Word2Vec Embedding\n",
    "\n",
    "Unlike TF-IDF, Word2Vec learns **dense vector representations** for each word based on its context in the training data.  \n",
    "This allows us to capture **semantic meaning**‚Äîwords with similar meaning tend to have similar vectors.\n",
    "\n",
    "We use Gensim's `Word2Vec` to:\n",
    "- Train an embedding model on our tokenized training data.\n",
    "- Convert each review into a sequence of word vectors using a helper function.\n",
    "\n",
    "### Why use Word2Vec?\n",
    "\n",
    "- It captures **semantic relationships** between words (e.g., king - man + woman ‚âà queen).\n",
    "- The vectors are **dense and compact**, suitable for neural networks.\n",
    "- It enables deep models like BiLSTM to process richer information than sparse counts.\n",
    "\n",
    "### Importance of Embedding in NLP:\n",
    "\n",
    "- Embedding layers transform words into a format the model can learn from.\n",
    "- Good embeddings boost model performance by providing meaningful context.\n",
    "- They serve as the **foundation of modern NLP architectures** (including Transformers, BERT, etc.).\n",
    "\n",
    "In short, Word2Vec gives your model a better understanding of language structure and meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff8b4b7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T00:34:24.208230Z",
     "iopub.status.busy": "2025-07-01T00:34:24.207961Z",
     "iopub.status.idle": "2025-07-01T00:35:08.092100Z",
     "shell.execute_reply": "2025-07-01T00:35:08.091358Z"
    },
    "papermill": {
     "duration": 43.892461,
     "end_time": "2025-07-01T00:35:08.094094",
     "exception": false,
     "start_time": "2025-07-01T00:34:24.201633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train a Word2Vec model on the tokenized text\n",
    "w2v_model = Word2Vec(sentences=train_df['tokens'], vector_size=200, window=6, min_count=2)\n",
    "\n",
    "# Function to convert tokens to sequence of word vectors\n",
    "def tokens_to_sequence(tokens):\n",
    "    vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
    "    return vectors\n",
    "\n",
    "# Apply tokens_to_sequence to convert tokens to sequences of word vectors\n",
    "X_train_seq_w2v = train_df['tokens'].apply(tokens_to_sequence).tolist()\n",
    "X_val_seq_w2v = val_df['tokens'].apply(tokens_to_sequence).tolist()\n",
    "X_test_seq_w2v = test_df['tokens'].apply(tokens_to_sequence).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c570d28",
   "metadata": {
    "papermill": {
     "duration": 0.004994,
     "end_time": "2025-07-01T00:35:08.104658",
     "exception": false,
     "start_time": "2025-07-01T00:35:08.099664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"text-align:center; background: #03045e; padding: 7px; border-radius:10px 10px; font-size: 1.5em; color: #e3f2fd; cursor: pointer;font-family: cursive;\"><b> 7. Model Training </b></div>\n",
    "\n",
    "A model is a mathematical structure that learns patterns from data.  \n",
    "Training a model means teaching it to make predictions by learning from labeled examples (inputs and outputs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491beb3f",
   "metadata": {
    "papermill": {
     "duration": 0.004914,
     "end_time": "2025-07-01T00:35:08.114709",
     "exception": false,
     "start_time": "2025-07-01T00:35:08.109795",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7.1 Logistic Regression\n",
    "\n",
    "We begin by training a simple yet powerful baseline model ‚Äî Logistic Regression ‚Äî using the TF-IDF features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452cbb1e",
   "metadata": {
    "papermill": {
     "duration": 0.004913,
     "end_time": "2025-07-01T00:35:08.124617",
     "exception": false,
     "start_time": "2025-07-01T00:35:08.119704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Logistic Regression** is a linear model used for binary classification tasks.  \n",
    "It predicts the probability that a given input belongs to a certain class (e.g., positive or negative).\n",
    "\n",
    "### Why use Logistic Regression in NLP?\n",
    "- It is **fast**, **interpretable**, and **easy to implement**.\n",
    "- It often provides **strong baselines** for text classification when combined with good feature extraction (like TF-IDF).\n",
    "- Despite being simple, it works surprisingly well on many real-world NLP tasks ‚Äî especially when:\n",
    "  - The dataset is not huge.\n",
    "  - Interpretability is important.\n",
    "  - You need fast prototyping or lightweight deployment.\n",
    "\n",
    "While LSTMs and Transformers are more powerful for context and deep semantics, Logistic Regression still holds value for:\n",
    "- Quick experimentation.\n",
    "- Low-resource environments.\n",
    "- Tasks where deep models are overkill.\n",
    "\n",
    "### Explanation of hyperparameters:\n",
    "- `max_iter=250`: The maximum number of iterations to converge (sometimes TF-IDF + large data needs more steps).\n",
    "- `C=0.1`: Inverse of regularization strength ‚Äî lower means stronger regularization (helps avoid overfitting).\n",
    "- `penalty='l2'`: L2 regularization adds a penalty for large weights (encourages simpler models).\n",
    "\n",
    "### Why evaluate on the validation set?\n",
    "\n",
    "Validation accuracy shows how well the model generalizes.  \n",
    "If it's much lower than training accuracy ‚Üí possible **overfitting**.  \n",
    "If both are close ‚Üí the model is likely generalizing well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f3ace27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T00:35:08.136016Z",
     "iopub.status.busy": "2025-07-01T00:35:08.135740Z",
     "iopub.status.idle": "2025-07-01T00:35:08.498302Z",
     "shell.execute_reply": "2025-07-01T00:35:08.497173Z"
    },
    "papermill": {
     "duration": 0.370433,
     "end_time": "2025-07-01T00:35:08.500133",
     "exception": false,
     "start_time": "2025-07-01T00:35:08.129700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy:\n",
      "Train Accuracy: 0.8768\n",
      "Validation Accuracy: 0.8698\n"
     ]
    }
   ],
   "source": [
    "# Initialize Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=250, C=0.1, penalty='l2')\n",
    "# Train the model\n",
    "log_reg.fit(X_train_tfidf, train_df['sentiment'])\n",
    "\n",
    "# Predict on training set\n",
    "log_train_preds = log_reg.predict(X_train_tfidf)\n",
    "train_accuracy = accuracy_score(train_df['sentiment'], log_train_preds)\n",
    "\n",
    "# Predict on validation set\n",
    "log_val_preds = log_reg.predict(X_val_tfidf)\n",
    "val_accuracy = accuracy_score(val_df['sentiment'], log_val_preds)\n",
    "\n",
    "# Print results\n",
    "print(\"Logistic Regression Accuracy:\")\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4906fea4",
   "metadata": {
    "papermill": {
     "duration": 0.005492,
     "end_time": "2025-07-01T00:35:08.511533",
     "exception": false,
     "start_time": "2025-07-01T00:35:08.506041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7.2 BiLSTM (Bidirectional LSTM)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are designed to handle sequential data.  \n",
    "Long Short-Term Memory (LSTM) is a special type of RNN that solves the vanishing gradient problem, allowing the model to learn long-term dependencies.\n",
    "\n",
    "**BiLSTM** extends LSTM by processing the input sequence in both forward and backward directions.  \n",
    "This gives the model more context, especially useful in NLP where both past and future words can influence meaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f4a61a",
   "metadata": {
    "papermill": {
     "duration": 0.005082,
     "end_time": "2025-07-01T00:35:08.521930",
     "exception": false,
     "start_time": "2025-07-01T00:35:08.516848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Sequence Padding\n",
    "\n",
    "Before feeding data into the BiLSTM model, we need to make sure that all input sequences have the same shape.  \n",
    "This is done using `pad_sequences()`:\n",
    "\n",
    "- `max_len = 280`: the maximum number of tokens per review.\n",
    "- `vector_size = 200`: the size of each word vector from Word2Vec.\n",
    "- Padding and truncation are applied **after** the sequence (`post`) for consistency.\n",
    "\n",
    "This step produces a 3D tensor of shape `(samples, max_len, vector_size)`, which is required for LSTM input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04b6a0a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T00:35:08.534544Z",
     "iopub.status.busy": "2025-07-01T00:35:08.533820Z",
     "iopub.status.idle": "2025-07-01T00:35:13.754821Z",
     "shell.execute_reply": "2025-07-01T00:35:13.753809Z"
    },
    "papermill": {
     "duration": 5.230087,
     "end_time": "2025-07-01T00:35:13.757538",
     "exception": false,
     "start_time": "2025-07-01T00:35:08.527451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define max sequence length and vector size (same as Word2Vec vector size)\n",
    "max_len = 280\n",
    "vector_size = 200\n",
    "\n",
    "# Pad sequences to make them all of the same shape (max_len x vector_size)\n",
    "X_train_seq_padded = pad_sequences(X_train_seq_w2v, maxlen=max_len,\n",
    "                                   dtype='float32',padding='post',\n",
    "                                   truncating='post', value=0.0)\n",
    "X_val_seq_padded = pad_sequences(X_val_seq_w2v, maxlen=max_len,\n",
    "                                 dtype='float32',padding='post',\n",
    "                                 truncating='post', value=0.0)\n",
    "X_test_seq_padded = pad_sequences(X_test_seq_w2v, maxlen=max_len,\n",
    "                                  dtype='float32',padding='post',\n",
    "                                  truncating='post', value=0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd0660",
   "metadata": {
    "papermill": {
     "duration": 0.008746,
     "end_time": "2025-07-01T00:35:13.775848",
     "exception": false,
     "start_time": "2025-07-01T00:35:13.767102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### BiLSTM Model Architecture\n",
    "\n",
    "The model is built using Keras `Sequential` API:\n",
    "\n",
    "- `Masking`: ignores padding (zeros) in the input.\n",
    "- `Bidirectional(LSTM)`: two stacked BiLSTM layers to capture forward and backward context.\n",
    "- `Dropout`: added after each layer to reduce overfitting.\n",
    "- `Dense` layers: fully connected layers for classification.\n",
    "- Final `Dense(1, activation='sigmoid')`: outputs probability of positive sentiment.\n",
    "\n",
    "We compile the model with:\n",
    "- `binary_crossentropy` loss (since it‚Äôs a binary classification task).\n",
    "- `Adam` optimizer with learning rate `0.001`.\n",
    "\n",
    "### Why BiLSTM for NLP?\n",
    "\n",
    "- Text is **sequential** ‚Äî the meaning of a word depends on its position and neighboring words.\n",
    "- LSTM captures dependencies from **past tokens** (left-to-right).\n",
    "- BiLSTM captures **both past and future tokens** (left-to-right and right-to-left).\n",
    "- This makes it more powerful than regular LSTM in tasks like sentiment analysis, named entity recognition, and more.\n",
    "\n",
    "While Transformers have surpassed BiLSTM in many benchmarks, BiLSTM still performs well, especially:\n",
    "- When data is not massive.\n",
    "- When model size or training time is limited.\n",
    "- In low-resource or real-time applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97530c23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T00:35:13.792053Z",
     "iopub.status.busy": "2025-07-01T00:35:13.791123Z",
     "iopub.status.idle": "2025-07-01T00:45:01.250159Z",
     "shell.execute_reply": "2025-07-01T00:45:01.249216Z"
    },
    "papermill": {
     "duration": 587.46846,
     "end_time": "2025-07-01T00:45:01.252287",
     "exception": false,
     "start_time": "2025-07-01T00:35:13.783827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/masking.py:47: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 103ms/step - accuracy: 0.7502 - loss: 0.9726 - val_accuracy: 0.8475 - val_loss: 0.5257\n",
      "Epoch 2/12\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 94ms/step - accuracy: 0.8548 - loss: 0.4926 - val_accuracy: 0.8406 - val_loss: 0.4448\n",
      "Epoch 3/12\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 93ms/step - accuracy: 0.8646 - loss: 0.4209 - val_accuracy: 0.8869 - val_loss: 0.3466\n",
      "Epoch 4/12\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 92ms/step - accuracy: 0.8751 - loss: 0.3650 - val_accuracy: 0.8839 - val_loss: 0.3334\n",
      "Epoch 5/12\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 92ms/step - accuracy: 0.8797 - loss: 0.3473 - val_accuracy: 0.8831 - val_loss: 0.3235\n",
      "Epoch 6/12\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 92ms/step - accuracy: 0.8894 - loss: 0.3232 - val_accuracy: 0.8844 - val_loss: 0.3189\n",
      "Epoch 7/12\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 92ms/step - accuracy: 0.8879 - loss: 0.3203 - val_accuracy: 0.8924 - val_loss: 0.3128\n",
      "Epoch 8/12\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 92ms/step - accuracy: 0.8911 - loss: 0.3087 - val_accuracy: 0.8945 - val_loss: 0.3058\n",
      "Epoch 9/12\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 92ms/step - accuracy: 0.8956 - loss: 0.3012 - val_accuracy: 0.8834 - val_loss: 0.3223\n",
      "Epoch 10/12\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 92ms/step - accuracy: 0.8988 - loss: 0.2962 - val_accuracy: 0.8827 - val_loss: 0.3259\n",
      "Epoch 11/12\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 93ms/step - accuracy: 0.8996 - loss: 0.2964 - val_accuracy: 0.8896 - val_loss: 0.3180\n",
      "Epoch 12/12\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 93ms/step - accuracy: 0.9027 - loss: 0.2909 - val_accuracy: 0.8930 - val_loss: 0.3201\n"
     ]
    }
   ],
   "source": [
    "# Build the BiLSTM model\n",
    "model = Sequential([\n",
    "    Masking(mask_value=0.0, input_shape=(max_len, vector_size)), \n",
    "    Bidirectional(LSTM(256, return_sequences=True, \n",
    "                       kernel_regularizer=regularizers.l2(0.0005))),\n",
    "    Dropout(0.4),\n",
    "    Bidirectional(LSTM(128, kernel_regularizer=regularizers.l2(0.0005))),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0005)),\n",
    "    Dropout(0.4),\n",
    "    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.0005)),\n",
    "    Dropout(0.4),\n",
    "    Dense(1, activation='sigmoid') \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer= Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_seq_padded,\n",
    "                    train_df['sentiment'],\n",
    "                    epochs=12,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(X_val_seq_padded, val_df['sentiment']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d82e4e",
   "metadata": {
    "papermill": {
     "duration": 0.298406,
     "end_time": "2025-07-01T00:45:01.893340",
     "exception": false,
     "start_time": "2025-07-01T00:45:01.594934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"text-align:center; background: #03045e; padding: 7px; border-radius:10px 10px; font-size: 1.5em; color: #e3f2fd; cursor: pointer;font-family: cursive;\"><b> 8. Export Predictions for Evaluation </b></div>\n",
    "\n",
    "After generating predictions and probabilities from both models, we save the results into two CSV files:\n",
    "\n",
    "- `logistic_preds.csv`: Contains predictions from the Logistic Regression model.\n",
    "- `bilstm_preds.csv`: Contains predictions from the BiLSTM model.\n",
    "\n",
    "Each file includes:\n",
    "- The original review text.\n",
    "- The true sentiment label.\n",
    "- The predicted label.\n",
    "- The probability for both negative and positive classes.\n",
    "\n",
    "These files will be used in a **separate notebook** titled:  \n",
    "**\"NLP with IMDB: Classic Models vs. Transformers\"**,  \n",
    "which is dedicated to analyzing and comparing the performance of different models (Logistic Regression, BiLSTM, and DeBERTa).\n",
    "\n",
    "We separated the notebooks because:\n",
    "- Running both BiLSTM and DeBERTa in the same Kaggle notebook caused memory/GPU issues.\n",
    "- We had to **restart the kernel** to free resources.\n",
    "- This modular approach ensures smooth execution and better resource management.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6e9dc8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T00:45:02.505974Z",
     "iopub.status.busy": "2025-07-01T00:45:02.504932Z",
     "iopub.status.idle": "2025-07-01T00:45:16.701575Z",
     "shell.execute_reply": "2025-07-01T00:45:16.700545Z"
    },
    "papermill": {
     "duration": 14.503862,
     "end_time": "2025-07-01T00:45:16.703386",
     "exception": false,
     "start_time": "2025-07-01T00:45:02.199524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved logistic_preds.csv\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step\n",
      "Saved bilstm_preds.csv\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Predictions\n",
    "logistic_probs = log_reg.predict_proba(X_test_tfidf)\n",
    "logistic_preds = log_reg.predict(X_test_tfidf)\n",
    "\n",
    "logistic_results_df = pd.DataFrame({\n",
    "    'review': test_df['review'].values,\n",
    "    'true_label': test_df['sentiment'].values,\n",
    "    'predicted_label': logistic_preds,\n",
    "    'prob_negative': logistic_probs[:, 0],\n",
    "    'prob_positive': logistic_probs[:, 1]\n",
    "})\n",
    "\n",
    "logistic_results_df.to_csv(\"logistic_preds.csv\", index=False)\n",
    "print(\"Saved logistic_preds.csv\")\n",
    "\n",
    "\n",
    "# BiLSTM Predictions\n",
    "bilstm_probs = model.predict(X_test_seq_padded)\n",
    "bilstm_preds = (bilstm_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "bilstm_results_df = pd.DataFrame({\n",
    "    'review': test_df['review'].values,\n",
    "    'true_label': test_df['sentiment'].values,\n",
    "    'predicted_label': bilstm_preds,\n",
    "    'prob_negative': 1 - bilstm_probs.flatten(),\n",
    "    'prob_positive': bilstm_probs.flatten()\n",
    "})\n",
    "\n",
    "bilstm_results_df.to_csv(\"bilstm_preds.csv\", index=False)\n",
    "print(\"Saved bilstm_preds.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb5a666",
   "metadata": {
    "papermill": {
     "duration": 0.302649,
     "end_time": "2025-07-01T00:45:17.350182",
     "exception": false,
     "start_time": "2025-07-01T00:45:17.047533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"text-align:center; background: #03045e; padding: 7px; border-radius:10px 10px; font-size: 1.5em; color: #e3f2fd; cursor: pointer;font-family: cursive;\"><b> 9. Conclusion </b></div>\n",
    "\n",
    "In this notebook, we explored two classic approaches to sentiment analysis using the IMDB movie reviews dataset:\n",
    "\n",
    "- **Logistic Regression** with TF-IDF features  \n",
    "- **BiLSTM** with Word2Vec embeddings\n",
    "\n",
    "We applied essential NLP steps ‚Äî including text preprocessing, tokenization, and feature extraction ‚Äî then trained both models and saved their predictions for further analysis.\n",
    "\n",
    "---\n",
    "\n",
    "Natural Language Processing (NLP) enables machines to understand and interpret human language, and even with relatively simple models, we can uncover meaningful insights from text data.  \n",
    "This notebook demonstrates that classical methods like Logistic Regression and BiLSTM still serve as strong baselines and foundational tools in modern NLP workflows.\n",
    "\n",
    "---\n",
    "\n",
    "In the next phase of this project, we will compare the results from these models with a modern Transformer-based model (**DeBERTa**) in a separate notebook:  \n",
    "**\"NLP with IMDB: Classic Models vs. Transformers\"**\n",
    "\n",
    "This comparative analysis will help us better understand how traditional approaches stack up against state-of-the-art deep learning models in real-world NLP applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f1953e",
   "metadata": {
    "papermill": {
     "duration": 0.294368,
     "end_time": "2025-07-01T00:45:17.941305",
     "exception": false,
     "start_time": "2025-07-01T00:45:17.646937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"text-align:center; background: #03045e; padding: 7px; border-radius:10px 10px; font-size: 1.5em; color: #e3f2fd; cursor: pointer;font-family: cursive;\"><b> End Code ‚ò∫ </b></div>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 134715,
     "sourceId": 320111,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 715.158969,
   "end_time": "2025-07-01T00:45:22.314516",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-01T00:33:27.155547",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
